<!DOCTYPE html>




<html lang="en">
  <head>
    <meta charset="utf-8" />
    
    <title>Optimizer using Gradient Descent &mdash; LTL 1.0.0-beta documentation</title>
    <meta name="description" content="">
    <meta name="author" content="">

    

<link rel="stylesheet" href="_static/css/basicstrap-base.css" type="text/css" />
<link rel="stylesheet" id="current-theme" href="_static/css/bootstrap3/bootstrap.min.css" type="text/css" />
<link rel="stylesheet" id="current-adjust-theme" type="text/css" />

<link rel="stylesheet" href="_static/css/font-awesome.min.css">

<style type="text/css">
  body {
    padding-top: 60px;
    padding-bottom: 40px;
  }
</style>

<link rel="stylesheet" href="_static/css/basicstrap.css" type="text/css" />
<link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
<script type="text/javascript">
  var DOCUMENTATION_OPTIONS = {
            URL_ROOT:    './',
            VERSION:     '1.0.0-beta',
            COLLAPSE_INDEX: false,
            FILE_SUFFIX: '.html',
            HAS_SOURCE:  true
  };
</script>
    <script type="text/javascript" src="_static/js/jquery.min.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/js/bootstrap3.min.js"></script>
<script type="text/javascript" src="_static/js/jquery.cookie.min.js"></script>
<script type="text/javascript" src="_static/js/basicstrap.js"></script>
<script type="text/javascript">
</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="top" title="LTL 1.0.0-beta documentation" href="index.html" />
    <link rel="up" title="Optimizers" href="ltl.optimizers.html" />
    <link rel="next" title="Optimizer using Grid Search" href="ltl.optimizers.gridsearch.html" />
    <link rel="prev" title="Optimizer using FACE" href="ltl.optimizers.face.html" /> 
  </head>
  <body role="document">
    <div id="navbar-top" class="navbar navbar-fixed-top navbar-default" role="navigation" aria-label="top navigation">
      <div class="container-fluid">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">LTL 1.0.0-beta documentation</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav navbar-right">
              <li class="dropdown visible-xs">
                <a role="button" id="localToc" data-toggle="dropdown" data-target="#" href="#">Table Of Contents <b class="caret"></b></a>
                <ul class="dropdown-menu localtoc sp-localtoc" role="menu" aria-labelledby="localToc">
                <ul>
<li><a class="reference internal" href="#">Optimizer using Gradient Descent</a><ul>
<li><a class="reference internal" href="#gradientdescentoptimizer">GradientDescentOptimizer</a></li>
<li><a class="reference internal" href="#classicgdparameters">ClassicGDParameters</a></li>
<li><a class="reference internal" href="#stochasticgdparameters">StochasticGDParameters</a></li>
<li><a class="reference internal" href="#adamparameters">AdamParameters</a></li>
<li><a class="reference internal" href="#rmspropparameters">RMSPropParameters</a></li>
</ul>
</li>
</ul>

                </ul>
              </li>

            
              <li><a href="ltl.optimizers.face.html" title="Optimizer using FACE" accesskey="P">previous </a></li>
              <li><a href="ltl.optimizers.gridsearch.html" title="Optimizer using Grid Search" accesskey="N">next </a></li>
              <li><a href="py-modindex.html" title="Python Module Index" >modules </a></li>
              <li><a href="genindex.html" title="General Index" accesskey="I">index </a></li>
              <li><a href="ltl.html" >API Reference</a></li>
              <li><a href="ltl.optimizers.html" accesskey="U">Optimizers</a></li>
            
            <li class="visible-xs"><a href="_sources/ltl.optimizers.gradientdescent.rst.txt" rel="nofollow">Show Source</a></li>

            <li class="visible-xs">
                <form class="search form-search form-inline navbar-form navbar-right sp-searchbox" action="search.html" method="get">
                  <div class="input-append input-group">
                    <input type="text" class="search-query form-control" name="q" placeholder="Search...">
                    <span class="input-group-btn">
                    <input type="submit" class="btn" value="Go" />
                    </span>
                  </div>
                  <input type="hidden" name="check_keywords" value="yes" />
                  <input type="hidden" name="area" value="default" />
                </form>
            </li>

            

          </ul>

        </div>
      </div>
    </div>
    

    <!-- container -->
    <div class="container-fluid">

      <!-- row -->
      <div class="row">
         
<div class="col-md-3 hidden-xs" id="sidebar-wrapper">
  <div class="sidebar hidden-xs" role="navigation" aria-label="main navigation">
<h3><a href="index.html">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html">Overview</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="ltl.html">API Reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ltl.optimizees.html">Optimizees</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="ltl.optimizers.html">Optimizers</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="ltl.optimizers.html#optimizer-base-module">Optimizer Base Module</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="ltl.optimizers.html#implemented-examples">Implemented Examples</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="ltl.optimizers.crossentropy.html">Optimizer using Cross Entropy</a></li>
<li class="toctree-l4"><a class="reference internal" href="ltl.optimizers.face.html">Optimizer using FACE</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">Optimizer using Gradient Descent</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#gradientdescentoptimizer">GradientDescentOptimizer</a></li>
<li class="toctree-l5"><a class="reference internal" href="#classicgdparameters">ClassicGDParameters</a></li>
<li class="toctree-l5"><a class="reference internal" href="#stochasticgdparameters">StochasticGDParameters</a></li>
<li class="toctree-l5"><a class="reference internal" href="#adamparameters">AdamParameters</a></li>
<li class="toctree-l5"><a class="reference internal" href="#rmspropparameters">RMSPropParameters</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="ltl.optimizers.gridsearch.html">Optimizer using Grid Search</a></li>
<li class="toctree-l4"><a class="reference internal" href="ltl.optimizers.evolution.html">Optimizer using Evolutionary Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ltl.optimizers.simulatedannealing.html">Optimizer using Simulated Annealing</a></li>
<li class="toctree-l4"><a class="reference internal" href="ltl.optimizers.evolutionstrategies.html">Optimizer using Evolution Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="ltl.optimizers.naturalevolutionstrategies.html">Optimizer using Natural Evolution Strategies</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="ltl.dataprocessing.html">Data Processing Utility Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltl.logging_tools.html">Logging Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltl.html#other-module-functions">Other module functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ltl-bin.html">LTL Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="indices.html">Indices and tables</a></li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="ltl.optimizers.face.html"
                        title="previous chapter">Optimizer using FACE</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="ltl.optimizers.gridsearch.html"
                        title="next chapter">Optimizer using Grid Search</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/ltl.optimizers.gradientdescent.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" role="search">
  <h3>Quick search</h3>
  <form class="search form-inline" action="search.html" method="get">
      <div class="input-append input-group">
        <input type="text" class="search-query form-control" name="q" placeholder="Search...">
        <span class="input-group-btn">
        <input type="submit" class="btn" value="Go" />
        </span>
      </div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
  </div>
</div> 
        

        <div class="col-md-9" id="content-wrapper">
          <div class="document" role="main">
            <div class="documentwrapper">
              <div class="bodywrapper">
                <div class="body">
                  
  <div class="section" id="optimizer-using-gradient-descent">
<h1>Optimizer using Gradient Descent<a class="headerlink" href="#optimizer-using-gradient-descent" title="Permalink to this headline">¶</a></h1>
<div class="section" id="gradientdescentoptimizer">
<h2>GradientDescentOptimizer<a class="headerlink" href="#gradientdescentoptimizer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer">
<em class="property">class </em><code class="descclassname">ltl.optimizers.gradientdescent.optimizer.</code><code class="descname">GradientDescentOptimizer</code><span class="sig-paren">(</span><em>traj</em>, <em>optimizee_create_individual</em>, <em>optimizee_fitness_weights</em>, <em>parameters</em>, <em>optimizee_bounding_func=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ltl/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="ltl.optimizers.html#ltl.optimizers.optimizer.Optimizer" title="ltl.optimizers.optimizer.Optimizer"><code class="xref py py-class docutils literal"><span class="pre">ltl.optimizers.optimizer.Optimizer</span></code></a></p>
<p>Class for a generic gradient descent solver.
In the pseudo code the algorithm does:</p>
<dl class="docutils">
<dt>For n iterations do:</dt>
<dd><ul class="first last simple">
<li>Explore the fitness of individuals in the close vicinity of the current one</li>
<li>Calculate the gradient based on these fitnesses.</li>
<li><dl class="first docutils">
<dt>Create the new ‘current individual’ by taking a step in the parameters space along the direction</dt>
<dd>of the largest ascent of the plane</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<p>NOTE: This expects all parameters of the system to be of floating point</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>traj</strong> (<a class="reference external" href="https://pypet.readthedocs.io/en/latest/pypetdoc/trajectorydoc.html#pypet.trajectory.Trajectory" title="(in pypet v0.4)"><em>Trajectory</em></a>) – Use this pypet trajectory to store the parameters of the specific runs. The parameters should be
initialized based on the values in <cite>parameters</cite></li>
<li><strong>optimizee_create_individual</strong> – Function that creates a new individual</li>
<li><strong>optimizee_fitness_weights</strong> – Fitness weights. The fitness returned by the Optimizee is multiplied by these values (one for each
element of the fitness vector)</li>
<li><strong>parameters</strong> – Instance of <a class="reference external" href="https://docs.python.org/3/library/collections.html#collections.namedtuple" title="(in Python v3.7)"><code class="xref py py-func docutils literal"><span class="pre">namedtuple()</span></code></a> <a class="reference internal" href="#ltl.optimizers.gradientdescent.optimizer.ClassicGDParameters" title="ltl.optimizers.gradientdescent.optimizer.ClassicGDParameters"><code class="xref py py-class docutils literal"><span class="pre">ClassicGDParameters</span></code></a>,
<a class="reference external" href="https://docs.python.org/3/library/collections.html#collections.namedtuple" title="(in Python v3.7)"><code class="xref py py-func docutils literal"><span class="pre">namedtuple()</span></code></a> <a class="reference internal" href="#ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters" title="ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters"><code class="xref py py-class docutils literal"><span class="pre">StochasticGDParameters</span></code></a>,
<a class="reference external" href="https://docs.python.org/3/library/collections.html#collections.namedtuple" title="(in Python v3.7)"><code class="xref py py-func docutils literal"><span class="pre">namedtuple()</span></code></a> <a class="reference internal" href="#ltl.optimizers.gradientdescent.optimizer.RMSPropParameters" title="ltl.optimizers.gradientdescent.optimizer.RMSPropParameters"><code class="xref py py-class docutils literal"><span class="pre">RMSPropParameters</span></code></a> or
<a class="reference external" href="https://docs.python.org/3/library/collections.html#collections.namedtuple" title="(in Python v3.7)"><code class="xref py py-func docutils literal"><span class="pre">namedtuple()</span></code></a> <a class="reference internal" href="#ltl.optimizers.gradientdescent.optimizer.AdamParameters" title="ltl.optimizers.gradientdescent.optimizer.AdamParameters"><code class="xref py py-class docutils literal"><span class="pre">AdamParameters</span></code></a> containing the
parameters needed by the Optimizer. The type of this parameter is used to select one of the GD variants.</li>
<li><strong>optimizee_bounding_func</strong> – This is a function that takes an individual as argument and returns another individual that is
within bounds (The bounds are defined by the function itself). If not provided, the individuals
are not bounded.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ltl/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.get_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters used for recorder
:return: Dictionary containing recorder parameters</p>
</dd></dl>

<dl class="method">
<dt id="ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.post_process">
<code class="descname">post_process</code><span class="sig-paren">(</span><em>traj</em>, <em>fitnesses_results</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ltl/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.post_process"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.post_process" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="ltl.optimizers.html#ltl.optimizers.optimizer.Optimizer.post_process" title="ltl.optimizers.optimizer.Optimizer.post_process"><code class="xref py py-meth docutils literal"><span class="pre">post_process()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.end">
<code class="descname">end</code><span class="sig-paren">(</span><em>traj</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ltl/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.end" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="ltl.optimizers.html#ltl.optimizers.optimizer.Optimizer.end" title="ltl.optimizers.optimizer.Optimizer.end"><code class="xref py py-meth docutils literal"><span class="pre">end()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_classic_gd">
<code class="descname">init_classic_gd</code><span class="sig-paren">(</span><em>parameters</em>, <em>traj</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ltl/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.init_classic_gd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_classic_gd" title="Permalink to this definition">¶</a></dt>
<dd><p>Classic Gradient Descent specific initializiation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>traj</strong> (<a class="reference external" href="https://pypet.readthedocs.io/en/latest/pypetdoc/trajectorydoc.html#pypet.trajectory.Trajectory" title="(in pypet v0.4)"><em>Trajectory</em></a>) – The :mod:’pypet’ trajectory on which the parameters should get stored.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_rmsprop">
<code class="descname">init_rmsprop</code><span class="sig-paren">(</span><em>parameters</em>, <em>traj</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ltl/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.init_rmsprop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_rmsprop" title="Permalink to this definition">¶</a></dt>
<dd><p>RMSProp specific initializiation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>traj</strong> (<a class="reference external" href="https://pypet.readthedocs.io/en/latest/pypetdoc/trajectorydoc.html#pypet.trajectory.Trajectory" title="(in pypet v0.4)"><em>Trajectory</em></a>) – The :mod:’pypet’ trajectory on which the parameters should get stored.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_adam">
<code class="descname">init_adam</code><span class="sig-paren">(</span><em>parameters</em>, <em>traj</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ltl/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.init_adam"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_adam" title="Permalink to this definition">¶</a></dt>
<dd><p>ADAM specific initializiation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>traj</strong> (<a class="reference external" href="https://pypet.readthedocs.io/en/latest/pypetdoc/trajectorydoc.html#pypet.trajectory.Trajectory" title="(in pypet v0.4)"><em>Trajectory</em></a>) – The :mod:’pypet’ trajectory on which the parameters should get stored.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_stochastic_gd">
<code class="descname">init_stochastic_gd</code><span class="sig-paren">(</span><em>parameters</em>, <em>traj</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ltl/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.init_stochastic_gd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_stochastic_gd" title="Permalink to this definition">¶</a></dt>
<dd><p>Stochastic Gradient Descent specific initializiation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>traj</strong> (<a class="reference external" href="https://pypet.readthedocs.io/en/latest/pypetdoc/trajectorydoc.html#pypet.trajectory.Trajectory" title="(in pypet v0.4)"><em>Trajectory</em></a>) – The :mod:’pypet’ trajectory on which the parameters should get stored.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.classic_gd_update">
<code class="descname">classic_gd_update</code><span class="sig-paren">(</span><em>traj</em>, <em>gradient</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ltl/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.classic_gd_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.classic_gd_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates the current individual using the classic Gradient Descent algorithm.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>traj</strong> (<a class="reference external" href="https://pypet.readthedocs.io/en/latest/pypetdoc/trajectorydoc.html#pypet.trajectory.Trajectory" title="(in pypet v0.4)"><em>Trajectory</em></a>) – The :mod:’pypet’ trajectory which contains the parameters 
required by the update algorithm</li>
<li><strong>gradient</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.15)"><em>ndarray</em></a>) – The gradient of the fitness curve, evaluated at the current individual</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.rmsprop_update">
<code class="descname">rmsprop_update</code><span class="sig-paren">(</span><em>traj</em>, <em>gradient</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ltl/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.rmsprop_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.rmsprop_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates the current individual using the RMSProp algorithm.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>traj</strong> (<a class="reference external" href="https://pypet.readthedocs.io/en/latest/pypetdoc/trajectorydoc.html#pypet.trajectory.Trajectory" title="(in pypet v0.4)"><em>Trajectory</em></a>) – The :mod:’pypet’ trajectory which contains the parameters 
required by the update algorithm</li>
<li><strong>gradient</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.15)"><em>ndarray</em></a>) – The gradient of the fitness curve, evaluated at the current individual</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.adam_update">
<code class="descname">adam_update</code><span class="sig-paren">(</span><em>traj</em>, <em>gradient</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ltl/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.adam_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.adam_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates the current individual using the ADAM algorithm.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>traj</strong> (<a class="reference external" href="https://pypet.readthedocs.io/en/latest/pypetdoc/trajectorydoc.html#pypet.trajectory.Trajectory" title="(in pypet v0.4)"><em>Trajectory</em></a>) – The :mod:’pypet’ trajectory which contains the parameters 
required by the update algorithm</li>
<li><strong>gradient</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.15)"><em>ndarray</em></a>) – The gradient of the fitness curve, evaluated at the current individual</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.stochastic_gd_update">
<code class="descname">stochastic_gd_update</code><span class="sig-paren">(</span><em>traj</em>, <em>gradient</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ltl/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.stochastic_gd_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.stochastic_gd_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates the current individual using a stochastic version of the gradient descent algorithm.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>traj</strong> (<a class="reference external" href="https://pypet.readthedocs.io/en/latest/pypetdoc/trajectorydoc.html#pypet.trajectory.Trajectory" title="(in pypet v0.4)"><em>Trajectory</em></a>) – The :mod:’pypet’ trajectory which contains the parameters 
required by the update algorithm</li>
<li><strong>gradient</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.15)"><em>ndarray</em></a>) – The gradient of the fitness curve, evaluated at the current individual</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="classicgdparameters">
<h2>ClassicGDParameters<a class="headerlink" href="#classicgdparameters" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="ltl.optimizers.gradientdescent.optimizer.ClassicGDParameters">
<em class="property">class </em><code class="descclassname">ltl.optimizers.gradientdescent.optimizer.</code><code class="descname">ClassicGDParameters</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.ClassicGDParameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><code class="xref py py-class docutils literal"><span class="pre">tuple</span></code></a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>learning_rate</strong> – The rate of learning per step of gradient descent</li>
<li><strong>exploration_step_size</strong> – The standard deviation of random steps used for finite difference gradient</li>
<li><strong>n_random_steps</strong> – The amount of random steps used to estimate gradient</li>
<li><strong>n_iteration</strong> – number of iteration to perform</li>
<li><strong>stop_criterion</strong> – Stop if change in fitness is below this value</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.ClassicGDParameters.exploration_step_size">
<code class="descname">exploration_step_size</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.ClassicGDParameters.exploration_step_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.ClassicGDParameters.learning_rate">
<code class="descname">learning_rate</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.ClassicGDParameters.learning_rate" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.ClassicGDParameters.n_iteration">
<code class="descname">n_iteration</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.ClassicGDParameters.n_iteration" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.ClassicGDParameters.n_random_steps">
<code class="descname">n_random_steps</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.ClassicGDParameters.n_random_steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.ClassicGDParameters.seed">
<code class="descname">seed</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.ClassicGDParameters.seed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.ClassicGDParameters.stop_criterion">
<code class="descname">stop_criterion</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.ClassicGDParameters.stop_criterion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="stochasticgdparameters">
<h2>StochasticGDParameters<a class="headerlink" href="#stochasticgdparameters" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters">
<em class="property">class </em><code class="descclassname">ltl.optimizers.gradientdescent.optimizer.</code><code class="descname">StochasticGDParameters</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><code class="xref py py-class docutils literal"><span class="pre">tuple</span></code></a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>learning_rate</strong> – The rate of learning per step of gradient descent</li>
<li><strong>stochastic_deviation</strong> – The standard deviation of the random vector used to perturbate the gradient</li>
<li><strong>stochastic_decay</strong> – The decay of the influence of the random vector that is added to the gradient 
(set to 0 to disable stochastic perturbation)</li>
<li><strong>exploration_step_size</strong> – The standard deviation of random steps used for finite difference gradient</li>
<li><strong>n_random_steps</strong> – The amount of random steps used to estimate gradient</li>
<li><strong>n_iteration</strong> – number of iteration to perform</li>
<li><strong>stop_criterion</strong> – Stop if change in fitness is below this value</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters.exploration_step_size">
<code class="descname">exploration_step_size</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters.exploration_step_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters.learning_rate">
<code class="descname">learning_rate</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters.learning_rate" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters.n_iteration">
<code class="descname">n_iteration</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters.n_iteration" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters.n_random_steps">
<code class="descname">n_random_steps</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters.n_random_steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters.seed">
<code class="descname">seed</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters.seed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters.stochastic_decay">
<code class="descname">stochastic_decay</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters.stochastic_decay" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters.stochastic_deviation">
<code class="descname">stochastic_deviation</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters.stochastic_deviation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters.stop_criterion">
<code class="descname">stop_criterion</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.StochasticGDParameters.stop_criterion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="adamparameters">
<h2>AdamParameters<a class="headerlink" href="#adamparameters" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="ltl.optimizers.gradientdescent.optimizer.AdamParameters">
<em class="property">class </em><code class="descclassname">ltl.optimizers.gradientdescent.optimizer.</code><code class="descname">AdamParameters</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.AdamParameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><code class="xref py py-class docutils literal"><span class="pre">tuple</span></code></a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>learning_rate</strong> – The rate of learning per step of gradient descent</li>
<li><strong>exploration_step_size</strong> – The standard deviation of random steps used for finite difference gradient</li>
<li><strong>n_random_steps</strong> – The amount of random steps used to estimate gradient</li>
<li><strong>first_order_decay</strong> – Specifies the amount of decay of the historic first order momentum per gradient descent step</li>
<li><strong>second_order_decay</strong> – Specifies the amount of decay of the historic second order momentum per gradient descent step</li>
<li><strong>n_iteration</strong> – number of iteration to perform</li>
<li><strong>stop_criterion</strong> – Stop if change in fitness is below this value</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.AdamParameters.exploration_step_size">
<code class="descname">exploration_step_size</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.AdamParameters.exploration_step_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.AdamParameters.first_order_decay">
<code class="descname">first_order_decay</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.AdamParameters.first_order_decay" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.AdamParameters.learning_rate">
<code class="descname">learning_rate</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.AdamParameters.learning_rate" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.AdamParameters.n_iteration">
<code class="descname">n_iteration</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.AdamParameters.n_iteration" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.AdamParameters.n_random_steps">
<code class="descname">n_random_steps</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.AdamParameters.n_random_steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.AdamParameters.second_order_decay">
<code class="descname">second_order_decay</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.AdamParameters.second_order_decay" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.AdamParameters.seed">
<code class="descname">seed</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.AdamParameters.seed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.AdamParameters.stop_criterion">
<code class="descname">stop_criterion</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.AdamParameters.stop_criterion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="rmspropparameters">
<h2>RMSPropParameters<a class="headerlink" href="#rmspropparameters" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="ltl.optimizers.gradientdescent.optimizer.RMSPropParameters">
<em class="property">class </em><code class="descclassname">ltl.optimizers.gradientdescent.optimizer.</code><code class="descname">RMSPropParameters</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.RMSPropParameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><code class="xref py py-class docutils literal"><span class="pre">tuple</span></code></a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>learning_rate</strong> – The rate of learning per step of gradient descent</li>
<li><strong>exploration_step_size</strong> – The standard deviation of random steps used for finite difference gradient</li>
<li><strong>n_random_steps</strong> – The amount of random steps used to estimate gradient</li>
<li><strong>momentum_decay</strong> – Specifies the decay of the historic momentum at each gradient descent step</li>
<li><strong>n_iteration</strong> – number of iteration to perform</li>
<li><strong>stop_criterion</strong> – Stop if change in fitness is below this value</li>
<li><strong>seed</strong> – The random seed used for random number generation in the optimizer</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.RMSPropParameters.exploration_step_size">
<code class="descname">exploration_step_size</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.RMSPropParameters.exploration_step_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.RMSPropParameters.learning_rate">
<code class="descname">learning_rate</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.RMSPropParameters.learning_rate" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.RMSPropParameters.momentum_decay">
<code class="descname">momentum_decay</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.RMSPropParameters.momentum_decay" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.RMSPropParameters.n_iteration">
<code class="descname">n_iteration</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.RMSPropParameters.n_iteration" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.RMSPropParameters.n_random_steps">
<code class="descname">n_random_steps</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.RMSPropParameters.n_random_steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.RMSPropParameters.seed">
<code class="descname">seed</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.RMSPropParameters.seed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="ltl.optimizers.gradientdescent.optimizer.RMSPropParameters.stop_criterion">
<code class="descname">stop_criterion</code><a class="headerlink" href="#ltl.optimizers.gradientdescent.optimizer.RMSPropParameters.stop_criterion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
</div>


                </div>
              </div>
            </div>
          </div>
        </div>
        
        
      </div><!-- /row -->

      <!-- row -->
      <div class="row footer-relbar">
<div id="navbar-related" class=" related navbar navbar-default" role="navigation" aria-label="related navigation">
  <div class="navbar-inner">
    <ul class="nav navbar-nav ">
        <li><a href="index.html">LTL 1.0.0-beta documentation</a></li>
    </ul>
<ul class="nav navbar-nav pull-right hidden-xs hidden-sm">
      
        <li><a href="ltl.optimizers.face.html" title="Optimizer using FACE" >previous</a></li>
        <li><a href="ltl.optimizers.gridsearch.html" title="Optimizer using Grid Search" >next</a></li>
        <li><a href="py-modindex.html" title="Python Module Index" >modules</a></li>
        <li><a href="genindex.html" title="General Index" >index</a></li>
        <li><a href="ltl.html" >API Reference</a></li>
        <li><a href="ltl.optimizers.html" >Optimizers</a></li>
        <li><a href="#">top</a></li> 
      
    </ul>
  </div>
</div>
      </div><!-- /row -->

      <!-- footer -->
      <footer role="contentinfo">
          &copy; Copyright 2017, Anand Subramoney.
        Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.6.2.
      </footer>
      <!-- /footer -->

    </div>
    <!-- /container -->

  </body>
</html>